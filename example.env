PERSIST_DIRECTORY=db
MODEL_TYPE=LlamaCpp
MODEL_PATH=/path/for/model
#best english embeddings model
#best italian efederici/sentence-it5-base
EMBEDDINGS_MODEL_NAME=all-mpnet-base-v2
MODEL_N_CTX=4096
N_GPU_LAYERS=12 # if it throws cuda-out of memory error, lower this
USE_MLOCK=1
TARGET_SOURCE_CHUNKS=8
N_BATCH=1024 # The number of tokens processed per request, increasing this value leads to an increase in resource utilisation. A lower value, on the other hand, slows down inference.